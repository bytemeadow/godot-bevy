name: benchmark comment

on:
  workflow_run:
    workflows: ["ci"]
    types:
      - completed

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  comment:
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'
    
    steps:
      - name: Download benchmark results
        id: download
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          name: benchmark-results
          path: benchmark-results/
      
      - name: Check if this is a PR with benchmark results
        id: check
        run: |
          # Check if artifacts were downloaded successfully
          if [ "${{ steps.download.outcome }}" != "success" ]; then
            echo "is_pr=false" >> $GITHUB_OUTPUT
            echo "üìã No benchmark artifacts found (CI may not have run benchmarks)"
            exit 0
          fi
          
          # Check for PR metadata in benchmark-artifacts subdirectory
          if [ -f "benchmark-results/benchmark-artifacts/pr_number.txt" ]; then
            echo "is_pr=true" >> $GITHUB_OUTPUT
            echo "pr_number=$(cat benchmark-results/benchmark-artifacts/pr_number.txt)" >> $GITHUB_OUTPUT
            echo "pr_sha=$(cat benchmark-results/benchmark-artifacts/pr_sha.txt)" >> $GITHUB_OUTPUT
            echo "‚úÖ Found PR metadata for PR #$(cat benchmark-results/benchmark-artifacts/pr_number.txt)"
          else
            echo "is_pr=false" >> $GITHUB_OUTPUT
            echo "üìã No PR metadata found - this was likely a main branch run"
          fi
      
      - name: Comment PR with results
        if: steps.check.outputs.is_pr == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Read benchmark results
            const resultsDir = 'benchmark-results/examples/perf-test/benchmark_results';
            if (!fs.existsSync(resultsDir)) {
              console.log('No benchmark results directory found');
              return;
            }
            
            const files = fs.readdirSync(resultsDir).filter(f => f.startsWith('full_comparison_')).sort().reverse();

            if (files.length === 0) {
              console.log('No benchmark result files found');
              return;
            }

            const resultsPath = path.join(resultsDir, files[0]);
            const data = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));

            const analysis = data.analysis || {};
            const ratios = analysis.performance_ratios || {};

            let comment = "## üìä Benchmark Results\n\n";

            // Simple regression check against baseline
            let regressionStatus = null;
            try {
              const baselineUrl = `https://raw.githubusercontent.com/${{ github.repository }}/main/examples/boids-perf-test/baseline.json`;
              const baselineResponse = await fetch(baselineUrl);
              
              if (baselineResponse.ok) {
                const baseline = await baselineResponse.json();
                
                // Check if this is a CI-generated baseline
                if (baseline.ci_metadata) {
                  console.log(`Found CI baseline from commit: ${baseline.ci_metadata.commit}`);
                  
                  // Compare performance vs baseline and show % differences
                  const currentResults = data.results || {};
                  
                  let performanceChanges = [];
                  
                  // Check each implementation and entity count
                  for (const impl of ["godot", "bevy"]) {
                    const currentImpl = currentResults[impl] || {};
                    const baselineImpl = baseline[impl] || {};
                    
                    for (const [count, data] of Object.entries(currentImpl)) {
                      if (baselineImpl[count]) {
                        const currentFps = data.avg_fps;
                        const baselineFps = baselineImpl[count].avg_fps;
                        const percentChange = ((currentFps - baselineFps) / baselineFps) * 100;
                        
                        performanceChanges.push({
                          impl,
                          count: parseInt(count),
                          currentFps: currentFps.toFixed(1),
                          baselineFps: baselineFps.toFixed(1),
                          percentChange: percentChange.toFixed(1)
                        });
                      }
                    }
                  }
                  
                  regressionStatus = { status: 'comparison', changes: performanceChanges };
                } else {
                  regressionStatus = 'no_ci_baseline';
                }
              } else {
                regressionStatus = 'no_baseline';
              }
            } catch (error) {
              console.log('Error fetching baseline:', error);
              regressionStatus = 'error';
            }


            if (Object.keys(ratios).length > 0) {
              comment += "| Entity Count | Godot FPS | +Bevy FPS | Speedup |\n";
              comment += "|--------------|-----------|----------|----------|\n";

              for (const [count, r] of Object.entries(ratios).sort((a, b) => Number(a[0]) - Number(b[0]))) {
                comment += `| ${Number(count).toLocaleString()} | ${r.godot_fps.toFixed(1)} | ${r.bevy_fps.toFixed(1)} | **${r.speedup.toFixed(2)}x** |\n`;
              }
            }

            if (analysis.summary) {
              const s = analysis.summary;
              comment += `\n**Average Speedup**: ${s.avg_speedup.toFixed(2)}x\n`;
              comment += `**Range**: ${s.min_speedup.toFixed(2)}x - ${s.max_speedup.toFixed(2)}x\n`;
            }

            // Add performance comparison vs baseline at the end
            if (regressionStatus?.status === 'comparison') {
              comment += "\n## üìà Performance vs Baseline\n\n";
              comment += "| Entity Count | Godot Change | Bevy Change |\n";
              comment += "|--------------|--------------|-------------|\n";
              
              // Get all unique entity counts and sort them
              const allCounts = [...new Set(regressionStatus.changes.map(c => c.count))].sort((a, b) => a - b);
              
              for (const count of allCounts) {
                const godotChange = regressionStatus.changes.find(c => c.impl === 'godot' && c.count === count);
                const bevyChange = regressionStatus.changes.find(c => c.impl === 'bevy' && c.count === count);
                
                const godotText = godotChange ? 
                  `${godotChange.currentFps} FPS (${godotChange.percentChange > 0 ? '+' : ''}${godotChange.percentChange}%)` : 
                  'N/A';
                const bevyText = bevyChange ? 
                  `${bevyChange.currentFps} FPS (${bevyChange.percentChange > 0 ? '+' : ''}${bevyChange.percentChange}%)` : 
                  'N/A';
                
                comment += `| ${count.toLocaleString()} | ${godotText} | ${bevyText} |\n`;
              }
            } else if (regressionStatus === 'no_baseline') {
              comment += "\n‚ÑπÔ∏è **Note**: No baseline available for comparison";
            } else if (regressionStatus === 'no_ci_baseline') {
              comment += "\n‚ÑπÔ∏è **Note**: Baseline not CI-generated";
            }

            comment += "\n\n<sub>ü§ñ Generated by godot-bevy CI</sub>";

            const prNumber = ${{ steps.check.outputs.pr_number }};

            // Find existing comment or create new one
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' && comment.body.includes('üìä Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment,
              });
              console.log('Updated existing benchmark comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: comment,
              });
              console.log('Created new benchmark comment');
            }
