name: benchmark comment

on:
  workflow_run:
    workflows: ["benchmarks"]
    types:
      - completed

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  comment:
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'

    steps:
      - name: Download benchmark results
        id: download
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          name: benchmark-results
          path: benchmark-results/

      - name: Check if this is a PR with benchmark results
        id: check
        run: |
          # Check if artifacts were downloaded successfully
          if [ "${{ steps.download.outcome }}" != "success" ]; then
            echo "is_pr=false" >> $GITHUB_OUTPUT
            echo "📋 No benchmark artifacts found"
            exit 0
          fi

          # Debug: List downloaded artifact structure
          echo "📂 Downloaded artifact structure:"
          find benchmark-results -type f -name "*.txt" -o -name "*.json" | head -20

          # Check for PR metadata in multiple locations (handles different artifact structures)
          if [ -f "benchmark-results/itest/benchmark-artifacts/pr_number.txt" ]; then
            echo "is_pr=true" >> $GITHUB_OUTPUT
            echo "pr_number=$(cat benchmark-results/itest/benchmark-artifacts/pr_number.txt)" >> $GITHUB_OUTPUT
            echo "✅ Found PR metadata for PR #$(cat benchmark-results/itest/benchmark-artifacts/pr_number.txt)"
          elif [ -f "benchmark-results/benchmark-artifacts/pr_number.txt" ]; then
            echo "is_pr=true" >> $GITHUB_OUTPUT
            echo "pr_number=$(cat benchmark-results/benchmark-artifacts/pr_number.txt)" >> $GITHUB_OUTPUT
            echo "✅ Found PR metadata for PR #$(cat benchmark-results/benchmark-artifacts/pr_number.txt)"
          elif [ -f "benchmark-results/pr_number.txt" ]; then
            echo "is_pr=true" >> $GITHUB_OUTPUT
            echo "pr_number=$(cat benchmark-results/pr_number.txt)" >> $GITHUB_OUTPUT
            echo "✅ Found PR metadata for PR #$(cat benchmark-results/pr_number.txt)"
          else
            echo "is_pr=false" >> $GITHUB_OUTPUT
            echo "📋 No PR metadata found - this was likely a main branch run"
          fi

      - name: Comment PR with results
        if: steps.check.outputs.is_pr == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Look for the itest comparison file which is uploaded directly
            let comparisonFile = null;

            // Check for itest/bench-results-comparison.json
            if (fs.existsSync('benchmark-results/itest/bench-results-comparison.json')) {
              comparisonFile = 'benchmark-results/itest/bench-results-comparison.json';
            } else if (fs.existsSync('benchmark-results/bench-results-comparison.json')) {
              comparisonFile = 'benchmark-results/bench-results-comparison.json';
            }

            if (!comparisonFile) {
              console.log('No comparison file found - looking for any JSON files...');
              // Debug: list all JSON files
              const findJsonFiles = (dir) => {
                const files = fs.readdirSync(dir);
                for (const file of files) {
                  const fullPath = path.join(dir, file);
                  if (fs.statSync(fullPath).isDirectory()) {
                    findJsonFiles(fullPath);
                  } else if (file.endsWith('.json')) {
                    console.log('Found JSON file:', fullPath);
                  }
                }
              };
              findJsonFiles('benchmark-results');
              return;
            }

            console.log('Reading comparison file:', comparisonFile);

            // Read comparison data
            const comparison = JSON.parse(fs.readFileSync(comparisonFile, 'utf8'));

            let comment = '## 📊 Benchmark Results\n\n';

            // Parse comparison data - the format from parse-bench-results.py
            if (comparison && comparison.comparisons) {
              comment += '| Benchmark | Current | Baseline | Change |\n';
              comment += '|-----------|---------|----------|--------|\n';

              for (const comp of comparison.comparisons) {
                let changeStr = '';
                let statusIcon = '⚪';

                // Use the status from the comparison or calculate it
                if (comp.status === 'new') {
                  changeStr = `🆕 *new*`;
                } else if (comp.change_pct !== null && comp.change_pct !== undefined) {
                  // Determine status based on percent change
                  if (comp.status === 'regression' || comp.change_pct > 10) {
                    statusIcon = '🔴'; // regression
                  } else if (comp.status === 'slower' || comp.change_pct > 5) {
                    statusIcon = '🟡'; // slower
                  } else if (comp.status === 'faster' || comp.change_pct < -5) {
                    statusIcon = '🟢'; // faster
                  } else {
                    statusIcon = '⚪'; // neutral
                  }

                  const sign = comp.change_pct > 0 ? '+' : '';
                  changeStr = `${statusIcon} ${sign}${comp.change_pct.toFixed(1)}%`;
                }

                // Use display values from the comparison
                const currentTime = comp.current ? comp.current.median_display : '-';
                const baselineTime = comp.baseline ? comp.baseline.median_display : '-';

                comment += `| ${comp.name} | ${currentTime} | ${baselineTime} | ${changeStr} |\n`;
              }

              // Summary
              let regressionCount = 0;
              let improvementCount = 0;

              for (const comp of comparison.comparisons) {
                if (comp.status === 'regression') {
                  regressionCount++;
                } else if (comp.status === 'faster') {
                  improvementCount++;
                }
              }

              comment += '\n';
              if (regressionCount > 0) {
                comment += `⚠️ **${regressionCount} regression(s)** detected\n`;
              }
              if (improvementCount > 0) {
                comment += `✨ **${improvementCount} improvement(s)** detected\n`;
              }
              if (regressionCount === 0 && improvementCount === 0) {
                comment += '✅ Performance is stable\n';
              }

              // Add metadata if available
              if (comparison.metadata) {
                comment += '\n';
                if (comparison.metadata.total_benchmarks) {
                  comment += `📈 Total benchmarks: ${comparison.metadata.total_benchmarks}\n`;
                }
                if (comparison.metadata.environment) {
                  comment += `🖥️ Environment: ${comparison.metadata.environment}\n`;
                }
              }

            } else {
              comment += '📋 No benchmark comparison data available\n';
            }

            comment += '\n<sub>🤖 Generated by godot-bevy CI</sub>';

            const prNumber = ${{ steps.check.outputs.pr_number }};

            // Find existing comment or create new one
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('📊 Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment,
              });
              console.log('Updated existing benchmark comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: comment,
              });
              console.log('Created new benchmark comment');
            }